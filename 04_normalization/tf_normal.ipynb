{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tf to normalize\n",
    "size = 100\n",
    "adv_n = np.random.rand(100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, std = [0.49677381416382793, 0.29401708396386794]\n"
     ]
    }
   ],
   "source": [
    "def stddev(v, mean):\n",
    "#     adv_std = tf.sqrt(tf.reduce_mean(tf.square(v - mean)))\n",
    "    adv_std = tf.sqrt(tf.reduce_mean(tf.squared_difference(v, mean)))\n",
    "    return adv_std\n",
    "\n",
    "adv_mean = tf.reduce_mean(adv_n)\n",
    "adv_std = stddev(adv_n, adv_mean)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"mean, std = %s\" % (sess.run([adv_mean, adv_std])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized mean, std = [3.2862601528904633e-16, 0.9999999999999998]\n"
     ]
    }
   ],
   "source": [
    "# after normalization to check gaussion\n",
    "adv_normal_n = (adv_n - adv_mean) / adv_std \n",
    "adv_normal_mean = tf.reduce_mean(adv_normal_n)\n",
    "adv_normal_std = stddev(adv_normal_n, adv_normal_mean)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"normalized mean, std = %s\" % (sess.run([adv_normal_mean, adv_normal_std])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, std = [0.4967738, 0.29401708]\n"
     ]
    }
   ],
   "source": [
    "adv_mean, adv_variance = tf.nn.moments(tf.constant(adv_n, dtype=tf.float32), axes=[0])\n",
    "adv_std = tf.sqrt(adv_variance)\n",
    "with tf.Session() as sess:\n",
    "    print(\"mean, std = %s\" % (sess.run([adv_mean, adv_std])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized mean, std = [1.4305114e-08, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# after normalization to check gaussion\n",
    "adv_normal_n = (adv_n - adv_mean) / adv_std \n",
    "adv_normal_mean = tf.reduce_mean(adv_normal_n)\n",
    "adv_normal_std = stddev(adv_normal_n, adv_normal_mean)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"normalized mean, std = %s\" % (sess.run([adv_normal_mean, adv_normal_std])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle adv_std $\\approx$ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate adv_std is very small\n",
    "# adv_std = tf.constant(1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized mean, std = [1.4305114e-08, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# after normalization to check gaussion\n",
    "adv_normal_n = tf.cond(adv_std < 1e-7, lambda: (adv_n - adv_mean), lambda: (adv_n - adv_mean) / adv_std)\n",
    "adv_normal_mean = tf.reduce_mean(adv_normal_n)\n",
    "adv_normal_std = stddev(adv_normal_n, adv_normal_mean)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"normalized mean, std = %s\" % (sess.run([adv_normal_mean, adv_normal_std])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Recall that the expression for the policy gradient PG is  \n",
    "    $$ PG = E_{\\tau} [\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) * (Q_t - b_t )] $$  \n",
    "where  \n",
    "    $ tau=(s_0, a_0, ...) $ is a trajectory,  \n",
    "    $ Q_t $ is the Q-value at time t, $Q^{\\pi}(s_t, a_t)$,  \n",
    "    and $ b_t $ is a baseline which may depend on $s_t$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global setting of Q_n\n",
    "N = 5\n",
    "tau_len = 100\n",
    "# list of 1D np.array - rewards\n",
    "re_n = list(np.random.rand(N, tau_len))\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.970299, 0.9801  , 0.99    , 1.      ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma ** np.array([3, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: trajectory-based PG \n",
    "            \n",
    "(reward_to_go = False)\n",
    "\n",
    "Instead of $Q^{\\pi}(s_t, a_t)$, we use the total discounted reward summed over \n",
    "entire trajectory (regardless of which time step the Q-value should be for). \n",
    "\n",
    "For this case, the policy gradient estimator is\n",
    "\n",
    "  $$ E_{\\tau} [\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) * Ret(\\tau)] $$\n",
    "\n",
    "where\n",
    "\n",
    "  $ Ret(\\tau) = \\sum_{t'=0}^T \\gamma^{t'} r_{t'} $.\n",
    "\n",
    "**Thus, you should compute**\n",
    "\n",
    "  $ Q_t = Ret(\\tau) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all reward step t=1:T, they have the same total rewards, just copy * len(re_tau)\n",
    "Q_n = np.concatenate([[sum(re_tau * (gamma ** np.arange(len(re_tau))[::-1]))] * len(re_tau) for re_tau in re_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_discount_rewards(rewards, gamma):\n",
    "    return sum((gamma**i) * rewards[i] for i in range(len(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_n1 = np.concatenate([[sum_discount_rewards(re_tau, gamma)] * len(re_tau)\n",
    "                    for re_tau in re_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       29.43651937, 29.43651937, 29.43651937, 29.43651937, 29.43651937,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       30.89331529, 30.89331529, 30.89331529, 30.89331529, 30.89331529,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       32.92603401, 32.92603401, 32.92603401, 32.92603401, 32.92603401,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       36.29058204, 36.29058204, 36.29058204, 36.29058204, 36.29058204,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337,\n",
       "       32.15012337, 32.15012337, 32.15012337, 32.15012337, 32.15012337])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       29.58063966, 29.58063966, 29.58063966, 29.58063966, 29.58063966,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 , 30.2478686 ,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       32.25744575, 32.25744575, 32.25744575, 32.25744575, 32.25744575,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       34.91777155, 34.91777155, 34.91777155, 34.91777155, 34.91777155,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064,\n",
       "       33.03201064, 33.03201064, 33.03201064, 33.03201064, 33.03201064])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gamma ** np.array(range(len(re_n[0]))[::-1])) == len(re_n[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: reward-to-go PG \n",
    "\n",
    "(reward_to_go = True)\n",
    "\n",
    "Here, you estimate $Q^{\\pi}(s_t, a_t)$ by the discounted sum of rewards starting\n",
    "from time step t. \n",
    "\n",
    "**Thus, you should compute**\n",
    "\n",
    "  $$ Q_t = \\sum_{t'=t}^T \\gamma^{(t'-t)} * r_{t'} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_n_index = [[(np.arange(len(re_tau)-start)) \\\n",
    "        for start in np.arange(len(re_tau))] \\\n",
    "       for re_tau in re_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_n_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_n_index[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten result to 500\n",
    "Q_n = np.concatenate([[sum(re_tau[::-1][:len(re_tau)-start] * (gamma ** np.arange(len(re_tau)-start))) \\\n",
    "        for start in np.arange(len(re_tau))] \\\n",
    "       for re_tau in re_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Q_n[99] == re_n[0][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_n[0][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify to be consistent with https://github.com/Kelym/DeepRL-UCB2017-Homework/blob/master/hw2/train_pg.py\n",
    "def discount_rewards_to_go(rewards, gamma):\n",
    "    res = [] \n",
    "    future_reward = 0\n",
    "    for r in reversed(rewards):\n",
    "        future_reward = future_reward * gamma + r\n",
    "        res.append(future_reward)\n",
    "    return res[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_n1 = np.concatenate([discount_rewards_to_go(re_tau, gamma) for re_tau in re_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_n1[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_n[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(q_n1) == len(Q_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
