{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.training.checkpointable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2d0bddaa756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msac\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/foundation/tensorflow_daily/07_stop_gradient_rl/nn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# from tensorflow.python.keras.engine.network import Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/foundation/tensorflow_daily/07_stop_gradient_rl/network.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpointable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheckpointable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpointable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_structures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpointable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheckpointable_layer_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.training.checkpointable'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import logz\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import nn\n",
    "from sac import SAC\n",
    "import utils\n",
    "\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAC(env_name, exp_name, seed, logdir, algorithm_set_params):\n",
    "    alpha = {\n",
    "        'Ant-v2': 0.1,\n",
    "        'HalfCheetah-v2': 0.2,\n",
    "        'Hopper-v2': 0.2,\n",
    "        'Humanoid-v2': 0.05,\n",
    "        'Walker2d-v2': 0.2,\n",
    "    }.get(env_name, 0.2)\n",
    "\n",
    "    algorithm_params = {\n",
    "        'alpha': alpha,\n",
    "        'batch_size': 256,\n",
    "        'discount': 0.99,\n",
    "        'learning_rate': 1e-3,\n",
    "        'reparameterize': False,\n",
    "        'tau': 0.01,\n",
    "        'epoch_length': 1000,\n",
    "        'n_epochs': 500,\n",
    "        'two_qf': False,\n",
    "    }\n",
    "    # override if key and value in algorithm_set_params isn't None\n",
    "    for key, value in algorithm_set_params.items():\n",
    "        if key and value and algorithm_params[key] != value:\n",
    "            algorithm_params[key] = value\n",
    "\n",
    "    sampler_params = {\n",
    "        'max_episode_length': 1000,\n",
    "        'prefill_steps': 1000,\n",
    "    }\n",
    "    replay_pool_params = {\n",
    "        'max_size': 1e6,\n",
    "    }\n",
    "\n",
    "    value_function_params = {\n",
    "        'hidden_layer_sizes': (128, 128),\n",
    "    }\n",
    "\n",
    "    q_function_params = {\n",
    "        'hidden_layer_sizes': (128, 128),\n",
    "    }\n",
    "\n",
    "    policy_params = {\n",
    "        'hidden_layer_sizes': (128, 128),\n",
    "    }\n",
    "\n",
    "    logz.configure_output_dir(logdir)\n",
    "    params = {\n",
    "        'exp_name': exp_name,\n",
    "        'env_name': env_name,\n",
    "        'algorithm_params': algorithm_params,\n",
    "        'sampler_params': sampler_params,\n",
    "        'replay_pool_params': replay_pool_params,\n",
    "        'value_function_params': value_function_params,\n",
    "        'q_function_params': q_function_params,\n",
    "        'policy_params': policy_params\n",
    "    }\n",
    "    logz.save_params(params)\n",
    "\n",
    "    env = gym.envs.make(env_name)\n",
    "    # Set random seeds\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    sampler = utils.SimpleSampler(**sampler_params)\n",
    "    replay_pool = utils.SimpleReplayPool(\n",
    "        observation_shape=env.observation_space.shape,\n",
    "        action_shape=env.action_space.shape,\n",
    "        **replay_pool_params)\n",
    "\n",
    "    q_function = nn.QFunction(name='q_function', **q_function_params)\n",
    "    if algorithm_params.get('two_qf', False):\n",
    "        q_function2 = nn.QFunction(name='q_function2', **q_function_params)\n",
    "    else:\n",
    "        q_function2 = None\n",
    "    value_function = nn.ValueFunction(\n",
    "        name='value_function', **value_function_params)\n",
    "    target_value_function = nn.ValueFunction(\n",
    "        name='target_value_function', **value_function_params)\n",
    "    policy = nn.GaussianPolicy(\n",
    "        action_dim=env.action_space.shape[0],\n",
    "        reparameterize=algorithm_params['reparameterize'],\n",
    "        **policy_params)\n",
    "\n",
    "    sampler.initialize(env, policy, replay_pool)\n",
    "\n",
    "    algorithm = SAC(**algorithm_params)\n",
    "\n",
    "    tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "    tf_config.gpu_options.allow_growth = True  # may need if using GPU\n",
    "    with tf.Session(config=tf_config):\n",
    "        algorithm.build(\n",
    "            env=env,\n",
    "            policy=policy,\n",
    "            q_function=q_function,\n",
    "            q_function2=q_function2,\n",
    "            value_function=value_function,\n",
    "            target_value_function=target_value_function)\n",
    "\n",
    "        for epoch in algorithm.train(sampler, n_epochs=algorithm_params.get('n_epochs', 1000)):\n",
    "            logz.log_tabular('Iteration', epoch)\n",
    "            for k, v in algorithm.get_statistics().items():\n",
    "                logz.log_tabular(k, v)\n",
    "            for k, v in replay_pool.get_statistics().items():\n",
    "                logz.log_tabular(k, v)\n",
    "            for k, v in sampler.get_statistics().items():\n",
    "                logz.log_tabular(k, v)\n",
    "            logz.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "n_experiments = 1\n",
    "env_name='HalfCheetah-v2'\n",
    "exp_name='reinf'\n",
    "args_seed = 1\n",
    "logdir = os.path.join(Path().resolve(), 'data/', exp_name)\n",
    "\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(n_experiments):\n",
    "    seed = args_seed + 10*e\n",
    "#     train_SAC(\n",
    "#         env_name=env_name,\n",
    "#         exp_name=exp_name,\n",
    "#         seed=seed,\n",
    "#         logdir=os.path.join(logdir, '%d' % seed),\n",
    "#         algorithm_set_params={\n",
    "#             'reparameterize': False,\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = {\n",
    "    'Ant-v2': 0.1,\n",
    "    'HalfCheetah-v2': 0.2,\n",
    "    'Hopper-v2': 0.2,\n",
    "    'Humanoid-v2': 0.05,\n",
    "    'Walker2d-v2': 0.2,\n",
    "}.get(env_name, 0.2)\n",
    "\n",
    "algorithm_params = {\n",
    "    'alpha': alpha,\n",
    "    'batch_size': 256,\n",
    "    'discount': 0.99,\n",
    "    'learning_rate': 1e-3,\n",
    "    'reparameterize': False,\n",
    "    'tau': 0.01,\n",
    "    'epoch_length': 1000,\n",
    "    'n_epochs': 500,\n",
    "    'two_qf': False,\n",
    "}\n",
    "    \n",
    "sampler_params = {\n",
    "    'max_episode_length': 1000,\n",
    "    'prefill_steps': 1000,\n",
    "}\n",
    "replay_pool_params = {\n",
    "    'max_size': 1e6,\n",
    "}\n",
    "\n",
    "value_function_params = {\n",
    "    'hidden_layer_sizes': (128, 128),\n",
    "}\n",
    "\n",
    "q_function_params = {\n",
    "    'hidden_layer_sizes': (128, 128),\n",
    "}\n",
    "\n",
    "policy_params = {\n",
    "    'hidden_layer_sizes': (128, 128),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'exp_name': exp_name,\n",
    "    'env_name': env_name,\n",
    "    'algorithm_params': algorithm_params,\n",
    "    'sampler_params': sampler_params,\n",
    "    'replay_pool_params': replay_pool_params,\n",
    "    'value_function_params': value_function_params,\n",
    "    'q_function_params': q_function_params,\n",
    "    'policy_params': policy_params\n",
    "}\n",
    "# logz.save_params(params)\n",
    "\n",
    "env = gym.envs.make(env_name)\n",
    "# Set random seeds\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "sampler = utils.SimpleSampler(**sampler_params)\n",
    "replay_pool = utils.SimpleReplayPool(\n",
    "    observation_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    **replay_pool_params)\n",
    "\n",
    "q_function = nn.QFunction(name='q_function', **q_function_params)\n",
    "if algorithm_params.get('two_qf', False):\n",
    "    q_function2 = nn.QFunction(name='q_function2', **q_function_params)\n",
    "else:\n",
    "    q_function2 = None\n",
    "value_function = nn.ValueFunction(\n",
    "    name='value_function', **value_function_params)\n",
    "target_value_function = nn.ValueFunction(\n",
    "    name='target_value_function', **value_function_params)\n",
    "policy = nn.GaussianPolicy(\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    reparameterize=algorithm_params['reparameterize'],\n",
    "    **policy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "_observations_ph = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(None, observation_dim),\n",
    "    name='observation',\n",
    ")\n",
    "_next_observations_ph = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(None, observation_dim),\n",
    "    name='next_observation',\n",
    ")\n",
    "_actions_ph = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(None, action_dim),\n",
    "    name='actions',\n",
    ")\n",
    "_rewards_ph = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(None, ),\n",
    "    name='rewards',\n",
    ")\n",
    "_terminals_ph = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(None, ),\n",
    "    name='terminals',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_observations_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_actions_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(q_function([_observations_ph, _actions_ph]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function(_observations_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, log_probs = policy(_observations_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
