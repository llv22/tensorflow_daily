{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(3)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dim = 3\n",
    "sample_N = 100\n",
    "Q_vs = np.random.rand(sample_N, action_dim)\n",
    "Q_values = tf.constant(Q_vs, dtype=tf.float32)\n",
    "Q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "Q_max_values = tf.reduce_max(Q_values, axis=-1)\n",
    "with tf.Session() as sess:\n",
    "    print(Q_max_values.eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_indexes = np.random.randint(action_dim, size=sample_N)\n",
    "action_indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_d = np.dstack((np.arange(sample_N), action_indexes))\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack([np.arange(sample_N), action_indexes])\n",
    "indices = np.array([[i, action] for i, action in enumerate(action_indexes)])\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(indices_d == indices)\n",
    "assert np.all(indices_d == np.stack((np.arange(sample_N), action_indexes), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    assert np.all(tf.stack((tf.range(sample_N), action_indexes), axis=-1).eval() == indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.gather_nd(Q_values, indices_d).eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done_mask_ph = np.random.rand(1, sample_N)\n",
    "done_mask_ph = done_mask_ph > 0.5\n",
    "done_mask_ph = done_mask_ph.astype(int)[0]\n",
    "done_mask_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(np.arange(action_dim * sample_N).reshape(-1, action_dim))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n",
      "axis=0 ==> (2, 3)\n",
      "1 3\n",
      "axis=1 ==> (100, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    for axis_index, shape in enumerate(inputs.shape):\n",
    "        print(axis_index, shape)\n",
    "        if axis_index == 1:\n",
    "            print(\"axis=%s ==> %s\" % (axis_index, tf.gather(inputs, [1], axis=axis_index).eval().shape))\n",
    "        else:\n",
    "            print(\"axis=%s ==> %s\" % (axis_index, tf.gather(inputs, [0, 2], axis=axis_index).eval().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "(100, 3)\n",
      "(100,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# using tf.one_hot for Q_values filter\n",
    "with tf.Session() as sess:\n",
    "    print(tf.one_hot(action_indexes, action_dim).eval().shape)\n",
    "#     print(tf.one_hot(action_indexes, action_dim) * Q_values)\n",
    "    print(tf.multiply(tf.one_hot(action_indexes, action_dim), Q_values).eval().shape)\n",
    "#     as tf.one_hot will remove all actions with zero values\n",
    "    print(tf.reduce_sum(tf.one_hot(action_indexes, action_dim) * Q_values, axis=-1).eval().shape)\n",
    "    print(tf.reduce_sum(tf.one_hot(action_indexes, action_dim) * Q_values, axis=0).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import os.path as osp\n",
    "from collections import namedtuple\n",
    "\n",
    "from atari_wrappers import *\n",
    "from dqn_utils import *\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        tf.set_random_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "def wrap_deepmind(env):\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ClippedRewardsWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "seed = random.randint(0, 9999)\n",
    "set_global_seeds(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "expt_dir = '/tmp/hw3_vid_dir2/'\n",
    "env = wrappers.Monitor(env, osp.join(expt_dir, \"gym\"), force=True)\n",
    "env = wrap_deepmind(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_size=1000000\n",
    "frame_history_len=4\n",
    "lander=False\n",
    "gamma=0.99\n",
    "grad_norm_clipping=10\n",
    "num_timesteps=2e8\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])\n",
    "\n",
    "# This is just a rough estimate\n",
    "num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "lr_multiplier = 1.0\n",
    "lr_schedule = PiecewiseSchedule([(0,                   1e-4 * lr_multiplier),\n",
    "                                 (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                 (num_iterations / 2,  5e-5 * lr_multiplier),],\n",
    "                                outside_value=5e-5 * lr_multiplier)\n",
    "optimizer_spec = OptimizerSpec(\n",
    "    constructor=tf.train.AdamOptimizer,\n",
    "    kwargs=dict(epsilon=1e-4),\n",
    "    lr_schedule=lr_schedule\n",
    ")\n",
    "\n",
    "def stopping_criterion(env, t):\n",
    "    # notice that here t is the number of steps of the wrapped env,\n",
    "    # which is different from the number of steps in the underlying env\n",
    "    return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "\n",
    "exploration_schedule = PiecewiseSchedule(\n",
    "    [\n",
    "        (0, 1.0),\n",
    "        (1e6, 0.1),\n",
    "        (num_iterations / 2, 0.01),\n",
    "    ], outside_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atari_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=512,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_func = atari_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: <class 'atari_wrappers.ClippedRewardsWrapper'> doesn't implement 'reset' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if len(env.observation_space.shape) == 1:\n",
    "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "    input_shape = env.observation_space.shape\n",
    "else:\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "obs_t_ph = tf.placeholder(tf.float32 if lander else tf.uint8, [None] + list(input_shape))\n",
    "# placeholder for current action\n",
    "act_t_ph = tf.placeholder(tf.int32,   [None])\n",
    "# placeholder for current reward\n",
    "rew_t_ph = tf.placeholder(tf.float32, [None])\n",
    "# placeholder for next observation (or state)\n",
    "obs_tp1_ph = tf.placeholder(tf.float32 if lander else tf.uint8, [None] + list(input_shape))\n",
    "done_mask_ph = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# casting to float on GPU ensures lower data transfer times.\n",
    "if lander:\n",
    "    obs_t_float = obs_t_ph\n",
    "    obs_tp1_float = obs_tp1_ph\n",
    "else:\n",
    "    obs_t_float   = tf.cast(obs_t_ph,   tf.float32) / 255.0\n",
    "    obs_tp1_float = tf.cast(obs_tp1_ph, tf.float32) / 255.0\n",
    "\n",
    "# YOUR CODE HERE - Problem 1.3 Implementation \n",
    "## 1. q_values network\n",
    "q_allaction_values = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "q_action_values = tf.reduce_sum(q_allaction_values * tf.one_hot(act_t_ph, num_actions), axis=-1)\n",
    "q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "\n",
    "## 2. target_q_value network\n",
    "q_prime_values = q_func(obs_tp1_float, num_actions, scope=\"target_q_func\", reuse=False)\n",
    "target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_q_func')\n",
    "y_values = rew_t_ph + gamma * (1 - done_mask_ph) * tf.reduce_max(q_prime_values, axis=-1)\n",
    "\n",
    "# for q_value and q_target_value's Bellman error\n",
    "total_error = huber_loss(q_action_values - y_values)\n",
    "\n",
    "######\n",
    "\n",
    "# construct optimization op (with gradient clipping)\n",
    "learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "optimizer = optimizer_spec.constructor(learning_rate=learning_rate, **optimizer_spec.kwargs)\n",
    "train_fn = minimize_and_clip(optimizer, total_error, var_list=q_func_vars, clip_val=grad_norm_clipping)\n",
    "\n",
    "# update_target_fn will be called periodically to copy Q network to target Q network\n",
    "update_target_fn = []\n",
    "for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                           sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "    update_target_fn.append(var_target.assign(var))\n",
    "update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "# construct the replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len, lander=lander)\n",
    "replay_buffer_idx = None\n",
    "\n",
    "###############\n",
    "# RUN ENV     #\n",
    "###############\n",
    "model_initialized = False\n",
    "num_param_updates = 0\n",
    "mean_episode_reward      = -float('nan')\n",
    "best_mean_episode_reward = -float('inf')\n",
    "last_obs = env.reset()\n",
    "log_every_n_steps = 10000\n",
    "\n",
    "start_time = None\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original obs_Q_input.shape =  (84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "last_obs = env.reset()\n",
    "replay_buffer_idx = replay_buffer.store_frame(last_obs)\n",
    "obs_Q_input = replay_buffer.encode_recent_observation()\n",
    "\n",
    "print(\"original obs_Q_input.shape = \", obs_Q_input.shape)\n",
    "# obs_Q_input = obs_Q_input.reshape([-1] + list(obs_Q_input.shape))\n",
    "# print(\"Q network input obs_Q_input.shape = \", obs_Q_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(lr_schedule.value(t))\n",
    "#     q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "#     initialize_interdependent_variables(sess, q_func_vars)\n",
    "#     print(sess.run(tf.argmax(q_allaction_values, axis=-1), feed_dict={obs_t_float:obs_Q_input[None]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PiecewiseSchedule \n",
    "estimate value by current step number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 0.0001), (5000000.0, 0.0001)), ((5000000.0, 0.0001), (12500000.0, 5e-05)), ((12500000.0, 5e-05), (25000000.0, 5e-05))]\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = PiecewiseSchedule([\n",
    "                                 (0,                   1e-4 * lr_multiplier),\n",
    "                                 (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                 (num_iterations / 4,  5e-5 * lr_multiplier),\n",
    "                                 (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                ],\n",
    "                                outside_value=5e-5 * lr_multiplier)\n",
    "print(list(zip(lr_schedule._endpoints[:-1], lr_schedule._endpoints[1:])))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
